{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45f47503",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\">Import Liberaries </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5016e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed3fb14",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\">Difine Path and Initialize Dictionary </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e62a9d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\Shehab\\\\Desktop\\\\NLP\\\\Dataset\"\n",
    "dic = {}\n",
    "for folder in os.listdir(path):\n",
    "    dic.update( {folder : []} )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f522fb3f",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\"> Fill the Dictionary with articles and remove first 2 lines</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c599ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for folder in os.listdir(path):\n",
    "    folder_path = os.path.join(path,folder)\n",
    "    for file in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path,file)\n",
    "        with open(file_path, 'r') as f:\n",
    "            contents = f.readlines()\n",
    "            contents = contents[2:]\n",
    "            tmp = dic[folder]\n",
    "            tmp.append(contents)\n",
    "            dic[folder] = tmp\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2497cb",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\">From Dictionary to Data Frames </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db0e42b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class is alt.atheism and its number is 0\n",
      "Class is comp.graphics and its number is 1\n",
      "Class is comp.os.ms-windows.misc and its number is 2\n",
      "Class is comp.sys.ibm.pc.hardware and its number is 3\n",
      "Class is comp.sys.mac.hardware and its number is 4\n",
      "Class is comp.windows.x and its number is 5\n",
      "Class is misc.forsale and its number is 6\n",
      "Class is rec.autos and its number is 7\n",
      "Class is rec.motorcycles and its number is 8\n",
      "Class is rec.sport.baseball and its number is 9\n",
      "Class is rec.sport.hockey and its number is 10\n",
      "Class is sci.crypt and its number is 11\n",
      "Class is sci.electronics and its number is 12\n",
      "Class is sci.med and its number is 13\n",
      "Class is sci.space and its number is 14\n",
      "Class is soc.religion.christian and its number is 15\n",
      "Class is talk.politics.guns and its number is 16\n",
      "Class is talk.politics.mideast and its number is 17\n",
      "Class is talk.politics.misc and its number is 18\n",
      "Class is talk.religion.misc and its number is 19\n"
     ]
    }
   ],
   "source": [
    "dfs=[]\n",
    "labels = []\n",
    "i =0\n",
    "for cls in dic:\n",
    "    tmp_dic = {\"article\" : dic[cls] , \"group\" : [i]*len(dic[cls])}\n",
    "    print(f\"Class is {cls} and its number is {i}\")\n",
    "    labels.append(cls)\n",
    "    i+=1\n",
    "    df = pd.DataFrame(tmp_dic)\n",
    "    dfs.append(df)\n",
    "    \n",
    "    \n",
    "all_dfs = pd.concat(dfs)\n",
    "all_dfs.to_csv(\"All.csv\" , index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf1bd3d",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\">Tokinize and preprocessing </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56e008fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = pd.read_csv(\"All.csv\")\n",
    "stop = list(stopwords.words('english'))\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"\\\\n\", \"\")\n",
    "    text = text.replace(\"\\\\\", \"\")\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    pattern = r\"^[a-z]+(-[a-z]+)*.?$\"\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if  re.match(pattern, token)]\n",
    "    tokens = [x for x in tokens if x not in stop]\n",
    "    \n",
    "    return list(tokens)\n",
    "    \n",
    "    \n",
    "all_dfs[\"article\"] = all_dfs[\"article\"].apply(tokenize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5ef07c",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\">Apply POS and Lemma </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4aec9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_map = {\n",
    "    \"N\": \"n\",\n",
    "    \"V\": \"v\",\n",
    "    \"R\": \"r\",\n",
    "    \"J\": \"a\"\n",
    "}\n",
    "\n",
    "\n",
    "def pos_lemma(tokens):\n",
    "    tags=pos_tag(tokens)\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    w_net_lemma=[]\n",
    "    for token ,tag in tags:\n",
    "        if(tag_map.get(tag[0]) is not None ):\n",
    "            w_net_lemma.append(lemmatizer.lemmatize(token,pos=tag_map[tag[0]]))\n",
    "        else:\n",
    "            w_net_lemma.append(lemmatizer.lemmatize(token,pos='n'))\n",
    "            \n",
    "    return list(w_net_lemma)\n",
    "        \n",
    "all_dfs[\"article\"] = all_dfs[\"article\"].apply(pos_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c93610",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\">Save Data Frame </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90083a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs.to_csv(\"All.csv\" , index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aho)",
   "language": "python",
   "name": "aho"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
